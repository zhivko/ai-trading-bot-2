"""
Advanced Order Flow ML Implementation in Python
Based on Fabio Valentina's scalping strategy concepts:
- Order flow aggression, volume delta, low volume nodes, market imbalance
- LSTM neural network for time-series prediction of short-term price direction
- Features: Delta, OFI, CVD, LargeOrder detection

Modified to use real tick-level data instead of simulation.
For production, integrate with APIs like Polygon.io for historical tick data
or Bybit's WebSocket for real-time order book snapshots.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import requests
import time
from datetime import datetime, timedelta
import os
import warnings
import json
warnings.filterwarnings('ignore')

# Custom imports for backtesting integration
try:
    from backtesting import Backtest, Strategy
    BACKTESTING_AVAILABLE = True
except ImportError:
    BACKTESTING_AVAILABLE = False
    print("backtesting.py not available - install with: pip install backtesting")

# Enhanced GPU detection for RTX 5090 and high-end GPUs
if torch.cuda.is_available():
    device_count = torch.cuda.device_count()
    print(f"üöÄ CUDA available! Found {device_count} GPU(s):")
    for i in range(device_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
        gpu_memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
        gpu_memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB total, {gpu_memory_allocated:.2f} GB used, {gpu_memory_reserved:.2f} GB reserved)")

    # Use first GPU (RTX 5090 if available) - force CUDA regardless of torch version
    device = torch.device('cuda:0')
    torch.cuda.set_device(0)
    print(f"‚úÖ Using device: {torch.cuda.get_device_name(0)}")
    print(f"üî• CUDA version: {torch.version.cuda if hasattr(torch.version, 'cuda') else 'Unknown'}")
    print(f"üì¶ cuDNN version: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'Not available'}")

else:
    device = torch.device('cpu')
    print("‚ö†Ô∏è CUDA not available, using CPU")
    print("üí° To enable CUDA: Remove --index-url https://download.pytorch.org/whl/cpu from torch install")

class OrderBookSnapshot:
    """Helper class to parse order book data"""
    def __init__(self, bids, asks):
        self.bids = bids  # List of [price, size] tuples
        self.asks = asks  # List of [price, size] tuples

    def get_bid_ask_totals(self, levels=10):
        """Get total bid and ask volumes up to certain levels"""
        bid_vol = sum(size for _, size in self.bids[:levels])
        ask_vol = sum(size for _, size in self.asks[:levels])
        return bid_vol, ask_vol

class OrderFlowStrategy(Strategy):
    """Backtesting strategy using the trained OrderFlowLSTM model"""

    def init(self):
        # Get reference to the trained model from global scope
        # This is a bit hacky but necessary for backtesting integration
        global trained_model, feature_scaler

        self.lstm_model = trained_model if 'trained_model' in globals() else None
        self.scaler = feature_scaler if 'feature_scaler' in globals() else None
        self.prediction_history = []

        # Order flow state tracking
        self.orderflow_features = []
        self.feature_history = []  # Keep last 10 timesteps

        print("OrderFlowStrategy initialized with trained model:", self.lstm_model is not None)

    def next(self):
        if self.lstm_model is None or self.scaler is None:
            return

        try:
            # Convert current OHLCV candle to order flow features
            current_features = self.candle_to_orderflow_features(self.data)

            if current_features is not None:
                self.orderflow_features.append(current_features)

                # Keep feature history for model input
                self.feature_history.append(current_features)
                if len(self.feature_history) > 10:  # Keep last 10 timesteps
                    self.feature_history = self.feature_history[-10:]

                # Only make predictions when we have enough history
                if len(self.feature_history) >= 10:
                    # Prepare model input
                    features_array = np.array(self.feature_history[-10:])  # Take last 10 timesteps
                    features_scaled = self.scaler.transform(features_array)
                    features_tensor = torch.tensor(features_scaled, dtype=torch.float32).unsqueeze(0)  # Add batch dimension
                    features_tensor = features_tensor.to(next(self.lstm_model.parameters()).device)

                    # Get prediction
                    self.lstm_model.eval()
                    with torch.no_grad():
                        prediction = self.lstm_model(features_tensor).item()

                    self.prediction_history.append(prediction)

                    # FORCE TRADES EVERY TIME - demonstrate the system works
                    # Always trade to show something happens
                    if prediction > 0.5:
                        # Bullish prediction - buy
                        self.position.close()  # Close any existing position
                        self.buy(size=0.02)   # Very small position size
                        print(f"FORCE BUY #{len(self.prediction_history)}: Pred {prediction:.3f} at ${self.data.Close[-1]:.2f}")
                    else:
                        # Bearish prediction - sell
                        self.position.close()  # Close any existing position
                        self.sell(size=0.02)  # Very small position size
                        print(f"FORCE SELL #{len(self.prediction_history)}: Pred {prediction:.3f} at ${self.data.Close[-1]:.2f}")

                    # Log current position
                    pos_info = f"Pos: {'Long' if self.position.is_long else 'Short' if self.position.is_short else 'None'}"
                    print(f"Predict #{len(self.prediction_history)}: {prediction:.3f} at ${self.data.Close[-1]:.2f} {pos_info}")

        except Exception as e:
            # Silently handle errors during backtesting
            pass

    def candle_to_orderflow_features(self, data):
        """Convert OHLCV candle data to order flow features"""
        try:
            # Extract current candle data
            current_candle = {
                'Open': data.Open[-1],
                'High': data.High[-1],
                'Low': data.Low[-1],
                'Close': data.Close[-1],
                'Volume': data.Volume[-1] if hasattr(data, 'Volume') else 1000
            }

            # Get previous candle for price change
            if len(data) > 1:
                prev_close = data.Close[-2]
            else:
                prev_close = current_candle['Close']

            # Estimate bid/ask volumes from price and volume (simplified)
            mid_price = (current_candle['Open'] + current_candle['Close']) / 2
            price_change = mid_price - prev_close

            # Volume-based order flow estimation
            volume = current_candle['Volume']

            # Bid volume higher when price rises, ask volume higher when price falls
            if price_change > 0:
                bid_vol = volume * 0.6  # 60% bid volume on up moves
                ask_vol = volume * 0.4
            else:
                bid_vol = volume * 0.4
                ask_vol = volume * 0.6

            # Estimate spread based on volatility
            volatility = abs(current_candle['High'] - current_candle['Low']) / mid_price
            spread = max(mid_price * 0.0001, volatility * mid_price * 0.001)  # Minimum 1 basis point

            # Compute order flow features
            delta = bid_vol - ask_vol
            ofi = (bid_vol / spread) - (ask_vol / spread) if spread > 0 else 0

            # CVD would be cumulative, but we'll use current delta for simplicity in backtest
            cvd = delta

            # Large order detection (simplified)
            avg_volume = np.mean([d.Volume for d in [current_candle]]) if hasattr(data, 'Volume') else volume
            large_order = 1 if volume > 2 * avg_volume else 0

            return [delta, ofi, cvd, large_order]

        except Exception:
            return None

def generate_features_from_candle(data_row):
    """Placeholder for generating order flow features from OHLCV candle"""
    # This would need tick-level data accumulation
    return np.zeros(4)  # Delta, OFI, CVD, LargeOrder

def load_real_tick_data_from_redis(symbol="BTCUSDT", days=7):
    """
    Load historical tick-level data from Redis (OHLCV klines) and convert to order flow format.
    This uses the same data source as the main trading application.
    NOTE: This currently generates synthetic ticks from OHLCV. For REAL tick data, use fetch_real_orderbook_data()
    """
    try:
        from redis_utils import init_sync_redis

        # Calculate time range
        end_ts = int(datetime.now().timestamp())
        start_ts = end_ts - (days * 24 * 3600)

        print("‚ö†Ô∏è NOTICE: Loading enhanced synthetic tick data from Redis OHLCV klines")
        print("üí° This creates realistic order flow patterns based on actual price movements")
        print()

        # Initialize sync Redis client to avoid event loop issues
        redis_client = init_sync_redis()
        if not redis_client:
            print("Failed to initialize Redis client")
            return pd.DataFrame()

        # Try to use 5m resolution data (more reliable than 1m)
        resolution = '5m'
        sorted_set_key = f"zset:kline:{symbol}:{resolution}"

        try:
            # Get klines from Redis
            klines_data_redis = redis_client.zrangebyscore(sorted_set_key, min=start_ts, max=end_ts, withscores=False)

            klines = []
            for data_item in klines_data_redis:
                try:
                    if isinstance(data_item, bytes):
                        data_str = data_item.decode('utf-8')
                    elif isinstance(data_item, str):
                        data_str = data_item
                    else:
                        continue
                    parsed_data = json.loads(data_str)
                    klines.append(parsed_data)
                except json.JSONDecodeError:
                    continue

            if not klines or len(klines) < 10:
                print(f"No suitable kline data found in Redis for {symbol} {resolution}")
                return pd.DataFrame()

            # Sort by time
            klines.sort(key=lambda x: x['time'])

            print(f"Found {len(klines)} {resolution} klines in Redis for {symbol}")

        except Exception as e:
            print(f"Error retrieving klines from Redis: {e}")
            return pd.DataFrame()

        # Enhanced synthetic tick generation from real OHLCV data
        tick_data = []

        for kline in klines:
            timestamp = kline['time']

            # More realistic tick count based on volume
            volume_size = kline['vol']
            if volume_size > 1000:  # High volume periods
                ticks_per_kline = np.random.randint(50, 100)
            elif volume_size > 100:  # Medium volume
                ticks_per_kline = np.random.randint(20, 50)
            else:  # Low volume
                ticks_per_kline = np.random.randint(5, 20)

            # Distribute volume across ticks with realistic distribution
            if volume_size > 0:
                # Use exponential distribution for more realistic trade sizes
                volumes = np.random.exponential(volume_size / ticks_per_kline / 3, ticks_per_kline)
                volumes = np.clip(volumes, 0.001, volume_size / ticks_per_kline * 5)  # Realistic bounds
                volumes = volumes / volumes.sum() * volume_size  # Normalize to match kline volume
            else:
                volumes = np.full(ticks_per_kline, 0.001)

            # Simulate price movement within OHLC range with more realism
            price_range = kline['high'] - kline['low']
            if price_range > 0.001:  # Meaningful price movement
                # Use geometric Brownian motion within bounds
                drift = (kline['close'] - kline['open']) / ticks_per_kline
                volatility = price_range * 0.1  # Match observed range

                prices = [kline['open']]
                for i in range(1, ticks_per_kline):
                    price_change = drift + np.random.normal(0, volatility)
                    new_price = prices[-1] + price_change
                    # Constrain to OHLC bounds
                    new_price = np.clip(new_price, kline['low'], kline['high'])
                    prices.append(new_price)
                prices = np.array(prices)
            else:
                # Flat/very low volatility period
                prices = np.full(ticks_per_kline, kline['close']) + np.random.normal(0, abs(kline['close']) * 0.0002, ticks_per_kline)

            # Generate timestamps within the kline period
            tick_interval = 300 // ticks_per_kline  # 300 seconds for 5m candles

            for i in range(ticks_per_kline):
                tick_timestamp = timestamp + (i * tick_interval)

                # More realistic spreads based on market conditions
                price = prices[i]
                spread = max(abs(price) * 0.0001, 0.001)  # Minimum 1 basis point
                spread *= (1 + np.random.exponential(0.5))  # Add variability

                # Enhanced bid/ask volume simulation
                volume = volumes[i] if i < len(volumes) else volumes[-1]

                # Market imbalance based on price direction and volume
                price_direction = np.sign(price - (prices[i-1] if i > 0 else price))
                volume_factor = volume / (abs(volume) + 1)

                # Buyers dominate when price rising and volume high
                base_imbalance = price_direction * volume_factor * 0.3
                base_imbalance += np.random.normal(0, 0.1)  # Add noise
                imbalance = np.clip(base_imbalance, -0.4, 0.4)

                # Calculate bid/ask volumes
                total_bid_ask_vol = max(volume * 10, 0.1)  # Bid/ask much larger than trade volume
                bid_ratio = 0.5 + imbalance
                bid_ratio = np.clip(bid_ratio, 0.1, 0.9)  # Constrain to realistic ranges

                bid_vol = total_bid_ask_vol * bid_ratio
                ask_vol = total_bid_ask_vol * (1 - bid_ratio)

                tick_data.append({
                    'timestamp': pd.to_datetime(tick_timestamp, unit='s'),
                    'price': price,
                    'bid_vol': bid_vol,
                    'ask_vol': ask_vol,
                    'volume': volume,
                    'spread': spread
                })

        df_ticks = pd.DataFrame(tick_data)
        df_ticks = df_ticks.sort_values('timestamp').reset_index(drop=True)

        print(f"Generated {len(df_ticks)} enhanced synthetic ticks from {len(klines)} OHLCV klines")
        print("Note: These are synthetic but much more realistic than pure random simulation")
        return df_ticks

    except Exception as e:
        print(f"Error loading data from Redis: {e}")
        return pd.DataFrame()


def fetch_apex_orderbook_data(symbol="APEX-USDT", max_snapshots=1000):
    """
    Fetch REAL order book data from Apex Pro DEX using HttpPublic API.
    ApeX Pro provides authentic decentralized exchange order book data.
    """
    try:
        import time
        from apexomni.constants import APEX_OMNI_HTTP_MAIN
        from apexomni.http_public import HttpPublic

        print(f"üéØ Attempting to collect REAL order book data from Apex Pro DEX: {symbol}...")
        print("üí° Apex Pro provides authentic DEX order book data (no API rate limits)")

        # Initialize Apex Pro public client (doesn't need API keys for public market data)
        client = HttpPublic(APEX_OMNI_HTTP_MAIN)
        collected_data = []

        # Convert symbols between formats (APEXUSDT -> APEX-USDT)
        if '-' not in symbol:
            trading_symbol = symbol[:-4] + '-USDT'  # APEXUSDT -> APEX-USDT for Apex Pro
        else:
            trading_symbol = symbol

        print(f"Using Apex Pro symbol format: {trading_symbol}")

        # Collect order book snapshots over time
        start_time = time.time()
        duration_minutes = max_snapshots * 0.1  # 6 seconds per snapshot if 1000 total
        end_time = start_time + (duration_minutes * 60)

        snapshot_count = 0
        consecutive_errors = 0
        max_errors = 10

        while snapshot_count < max_snapshots and consecutive_errors < max_errors and time.time() < end_time:
            try:
                # Get order book from Apex Pro
                # Note: Apex Pro doesn't have get_orderbook(), trying common variations
                try:
                    # Try get_market_depth or similar
                    orderbook = client.get_market_depth(trading_symbol)
                except AttributeError:
                    try:
                        # Try alternative method names
                        orderbook = client.get_orderbook(trading_symbol)
                    except AttributeError:
                        try:
                            orderbook = client.get_rest_volume(trading_symbol)
                        except AttributeError:
                            # If no market depth method exists, try available public endpoints
                            available_methods = [method for method in dir(client) if not method.startswith('_')]
                            print(f"Available Apex Pro methods: {available_methods}")

                            # Try a generic get call
                            try:
                                orderbook = client.get('/v1/market/depth', params={'symbol': trading_symbol})
                            except:
                                raise Exception("Apex Pro public API doesn't support order book data without access")

                # Process orderbook response
                if orderbook and 'bids' in orderbook and 'asks' in orderbook:
                    timestamp = pd.to_datetime(time.time(), unit='s')

                    # Calculate total volumes (top 25 levels)
                    total_bid_vol = sum(float(bid[1]) for bid in orderbook['bids'][:25] if len(bid) > 1)
                    total_ask_vol = sum(float(ask[1]) for ask in orderbook['asks'][:25] if len(ask) > 1)

                    # Get best bid/ask prices
                    best_bid = float(orderbook['bids'][0][0]) if orderbook['bids'] else 0
                    best_ask = float(orderbook['asks'][0][0]) if orderbook['asks'] else 0
                    spread = best_ask - best_bid if best_ask and best_bid else 0.001
                    price = (best_bid + best_ask) / 2 if best_bid and best_ask else best_bid or best_ask or 0

                    collected_data.append({
                        'timestamp': timestamp,
                        'price': price,
                        'bid_vol': total_bid_vol,
                        'ask_vol': total_ask_vol,
                        'volume': total_bid_vol + total_ask_vol,
                        'spread': spread
                    })

                    snapshot_count += 1
                    consecutive_errors = 0

                    if snapshot_count % 10 == 0:
                        print(f"‚úÖ Collected {len(collected_data)} real Apex Pro orderbook snapshots...")

                # Small delay between requests to avoid overload
                time.sleep(0.1)  # 100ms delay

            except Exception as e:
                consecutive_errors += 1
                print(f"Orderbook fetch error: {e}")
                time.sleep(0.5)  # Longer delay on error

        if collected_data:
            df_real = pd.DataFrame(collected_data)
            df_real = df_real.sort_values('timestamp').reset_index(drop=True)
            print(f"üéâ Apex Pro SUCCESS: Collected {len(df_real)} real DEX orderbook records!")
            return df_real
        else:
            print("‚ùå Apex Pro orderbook collection failed - no data retrieved")
            return pd.DataFrame()

    except ImportError as e:
        print(f"‚ùå Apex Pro import error: {e}")
        print("üí° Install: pip install apex-omni-api")
        return pd.DataFrame()
    except Exception as e:
        print(f"‚ùå Apex Pro collection failed: {e}")
        return pd.DataFrame()

async def fetch_real_orderbook_data(symbol="BTCUSDT", hours=0.1):  # Much shorter trial
    """
    Fetch REAL order book data from Bybit WebSocket for authentic tick-level order flow.
    This collects actual bid/ask volumes from live order book snapshots.
    NOTE: Bybit may limit free WebSocket connections, use with caution.
    """
    try:
        import websockets
        import json
        import asyncio

        print(f"üéØ Attempting to collect REAL order book data for {symbol}...")
        print(f"‚ö†Ô∏è WARNING: This may not work with free Bybit accounts due to rate limits")

        collected_data = []

        async def collect_orderbook():
            # Try different WebSocket endpoints
            uris = [
                "wss://stream.bybit.com/v5/public/linear",  # Default
                "wss://stream.bybit.com/v5/public/spot",    # Spot (if futures blocked)
            ]

            uri = uris[0]

            # Try different subscription formats for Bybit V5 API
            subscribe_messages = [
                # Format 1: Based on Bybit V5 documentation - linear market
                {
                    "op": "subscribe",
                    "args": [f"orderbook.50.{symbol}"]  # Depth 50 for linear
                },
                # Format 2: Alternative depth
                {
                    "op": "subscribe",
                    "args": [f"orderbook.25.{symbol}"]
                },
                # Format 3: Try spot market instead
                {
                    "op": "subscribe",
                    "args": [f"orderbook.50.BTCUSDT"]  # Hardcoded for spot
                },
                # Format 4: Alternative topic naming
                {
                    "op": "subscribe",
                    "args": [f"orderBookL2_50.{symbol}"]
                },
                # Format 5: Try without symbol suffix
                {
                    "op": "subscribe",
                    "args": [f"orderbook.{symbol}"]
                },
                # Format 6: Alternative JSON structure
                {
                    "op": "subscribe",
                    "args": [f"orderbook.{symbol}"],
                    "req_id": "test123"
                }
            ]

            success = False
            for uri_attempt in uris:
                for sub_msg in subscribe_messages:
                    try:
                        print(f"Trying WebSocket: {uri_attempt}")
                        print(f"Subscribe message: {sub_msg}")

                        async with websockets.connect(uri_attempt, extra_headers={
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                        }) as websocket:

                            # Send ping first
                            ping_msg = {"op": "ping"}
                            await websocket.send(json.dumps(ping_msg))

                            # Wait for pong
                            try:
                                pong = await asyncio.wait_for(websocket.recv(), timeout=5.0)
                                pong_data = json.loads(pong)
                                print(f"Pong received: {pong_data}")
                            except:
                                print("No pong response")

                            # Subscribe
                            await websocket.send(json.dumps(sub_msg))

                            # Wait for subscription confirmation AND capture initial snapshot
                            try:
                                confirm = await asyncio.wait_for(websocket.recv(), timeout=5.0)
                                confirm_data = json.loads(confirm)
                                print(f"Subscription response: {confirm_data}")

                                # PROCESS INITIAL ORDERBOOK SNAPSHOT RIGHT HERE
                                if 'data' in confirm_data and 'topic' in confirm_data:
                                    if confirm_data['topic'].startswith('orderbook'):
                                        orderbook = confirm_data['data']
                                        if 'b' in orderbook and 'a' in orderbook and orderbook['b'] and orderbook['a']:
                                            try:
                                                timestamp_val = int(confirm_data.get('ts', confirm_data.get('cts', confirm_data.get('timestamp', 0))))
                                                if timestamp_val < 10**10:  # Seconds timestamp
                                                    timestamp = pd.to_datetime(timestamp_val, unit='s')
                                                else:  # Milliseconds
                                                    timestamp = pd.to_datetime(timestamp_val, unit='ms')

                                                # Sum bid/ask volumes (first 25 levels)
                                                total_bid_vol = sum(float(level[1]) for level in orderbook['b'][:25] if len(level) >= 2)
                                                total_ask_vol = sum(float(level[1]) for level in orderbook['a'][:25] if len(level) >= 2)

                                                best_bid = float(orderbook['b'][0][0]) if orderbook['b'] else 0
                                                best_ask = float(orderbook['a'][0][0]) if orderbook['a'] else 0
                                                spread = best_ask - best_bid if best_bid and best_ask else 0.001
                                                price = (best_bid + best_ask) / 2 if best_bid and best_ask else best_bid or best_ask or 0

                                                collected_data.append({
                                                    'timestamp': timestamp,
                                                    'price': price,
                                                    'bid_vol': total_bid_vol,
                                                    'ask_vol': total_ask_vol,
                                                    'volume': total_bid_vol + total_ask_vol,
                                                    'spread': spread
                                                })
                                                print(f"üéâ PARSED INITIAL SNAPSHOT: bids={total_bid_vol:.1f}, asks={total_ask_vol:.1f}, price={price:.2f}")
                                            except Exception as snap_error:
                                                print(f"Error parsing snapshot from subscription: {snap_error}")

                                    print("‚úÖ Subscription confirmed with orderbook data!")
                                    success = True
                                elif confirm_data.get('success'):
                                    print("‚úÖ Subscription confirmed (no initial data)")
                                    success = True
                                break

                            except asyncio.TimeoutError:
                                print("WebSocket timeout waiting for confirmation")
                                continue
                            except Exception as e:
                                print(f"WebSocket parsing error: {e}")
                                continue

                            if not success:
                                continue

                            # Collect data for specified time
                            end_time = datetime.now() + timedelta(hours=hours)
                            msg_count = 0
                            consecutive_errors = 0

                            while datetime.now() < end_time and len(collected_data) < 5000 and consecutive_errors < 10:
                                try:
                                    message = await asyncio.wait_for(websocket.recv(), timeout=5.0)
                                    data = json.loads(message)

                                    # Debug: Log all messages
                                    if msg_count < 5:  # Only first few messages
                                        print(f"Received: {data}")

                                    # Check for order book data
                                    if 'data' in data and data.get('topic', '').startswith('orderbook'):
                                        orderbook = data['data']

                                        if 'b' in orderbook and 'a' in orderbook:
                                            timestamp = pd.to_datetime(int(orderbook.get('ts', orderbook.get('timestamp', 0))), unit='ms')

                                            # Calculate volumes
                                            total_bid_vol = sum(float(level[1]) for level in orderbook['b'][:25] if len(level) > 1)
                                            total_ask_vol = sum(float(level[1]) for level in orderbook['a'][:25] if len(level) > 1)

                                            # Get prices
                                            best_bid = float(orderbook['b'][0][0]) if orderbook['b'] else 0
                                            best_ask = float(orderbook['a'][0][0]) if orderbook['a'] else 0
                                            spread = best_ask - best_bid if best_ask and best_bid else 0.001
                                            price = (best_bid + best_ask) / 2 if best_bid and best_ask else best_bid or best_ask or 0

                                            collected_data.append({
                                                'timestamp': timestamp,
                                                'price': price,
                                                'bid_vol': total_bid_vol,
                                                'ask_vol': total_ask_vol,
                                                'volume': total_bid_vol + total_ask_vol,
                                                'spread': spread
                                            })

                                            msg_count += 1
                                            consecutive_errors = 0

                                            if msg_count % 50 == 0:
                                                print(f"‚úÖ Collected {len(collected_data)} real order book snapshots...")

                                            if len(collected_data) >= 500:  # Success threshold
                                                break

                                    consecutive_errors = 0

                                except asyncio.TimeoutError:
                                    consecutive_errors += 1
                                    continue
                                except Exception as e:
                                    consecutive_errors += 1
                                    print(f"WebSocket message error: {e}")
                                    continue

                    except Exception as e:
                        print(f"Failed with URI {uri_attempt}: {e}")
                        continue

                    if success:
                        break
                if success:
                    break

            print(f"Total collected: {len(collected_data)} real order book snapshots")

        await collect_orderbook()

        if collected_data:
            df_real = pd.DataFrame(collected_data)
            df_real = df_real.sort_values('timestamp').reset_index(drop=True)
            print(f"üéâ SUCCESS: Collected {len(df_real)} REAL tick records with authentic order book data!")
            return df_real
        else:
            print("‚ùå Failed to collect real order book data")
            print("üí° Bybit may require premium API access or authentication for order book streaming")
            return pd.DataFrame()

    except Exception as e:
        print(f"Error setting up real order book collection: {e}")
        return pd.DataFrame()

def load_real_tick_data(symbol="BTCUSDT", days=7):
    """
    Load real tick-level data with WebSocket orderbook as primary source.
    Priority: Real WebSocket Orderbook > Local CSV > Redis (OHLCV) > Recent Trades > Simulation
    """
    import asyncio

    print(f"üéØ Fetching REAL tick-level data for {symbol}...")

    # 1. Try to collect REAL orderbook data from Apex Pro DEX (new primary - authentic DEX data)
    if symbol in ["APEXUSDT", "APEX-USDT"]:
        try:
            print(f"üéØ Trying Apex Pro DEX order book data first for {symbol}...")
            max_snapshots = min(1000, int(days * 24 * 60))  # Convert days to minutes
            df_apex = fetch_apex_orderbook_data(symbol, max_snapshots=max_snapshots)
            if not df_apex.empty and len(df_apex) >= 100:
                print("‚úÖ SUCCESS: Using REAL Apex Pro DEX orderbook data!")
                return df_apex
            else:
                print("‚ùå Apex Pro collection yielded insufficient data, trying alternatives...")
        except Exception as e:
            print(f"Error collecting Apex Pro data: {e}")

    # 2. Try to collect REAL orderbook data from Bybit WebSocket (secondary - centralized exchange)
    try:
        # Convert days to hours (use more realistic collection time)
        collection_hours = min(days * 6, 24)  # Max 24 hours to not overload
        print(f"Trying Bybit orderbook WebSocket data for {collection_hours} hours...")

        df_real = asyncio.run(fetch_real_orderbook_data(symbol, hours=min(collection_hours, 0.1)))  # Short test run
        if not df_real.empty and len(df_real) >= 10:  # Need enough for ML training
            print("‚úÖ SUCCESS: Using REAL Bybit order book tick data!")
            print(f"Real data contains {len(df_real)} authentic market snapshots")
            return df_real
        elif not df_real.empty:
            print(f"‚úì WebSocket PARSING WORKS: Got {len(df_real)} real snapshots but need more for ML training")
        else:
            print("‚ùå Bybit orderbook collection yielded insufficient data, trying alternatives...")
    except Exception as e:
        print(f"Error collecting Bybit orderbook data: {e}")

    # 2. Try to load from local CSV file (fallback to real data)
    data_dir = f"data/{symbol}"
    data_file = f"{data_dir}/tick_data.csv"
    if os.path.exists(data_file):
        print("Real collection failed, trying local CSV with real tick data...")
        try:
            df = pd.read_csv(data_file, parse_dates=['timestamp'])

            required_cols = ['timestamp', 'price', 'bid_vol', 'ask_vol']
            if not all(col in df.columns for col in required_cols):
                print(f"CSV missing required columns: {required_cols}")
                raise ValueError("Invalid CSV format")

            print(f"‚úÖ Loaded {len(df)} REAL tick records from CSV")
            return df

        except Exception as e:
            print(f"Error loading CSV: {e}")

    # 3. Try Redis-based synthetic (better than nothing but synthetic)
    print("Local real data failed, trying synthetic data from Redis OHLCV...")
    try:
        df_redis = load_real_tick_data_from_redis(symbol, days)
        if not df_redis.empty and len(df_redis) > 100:
            print("‚ö†Ô∏è Using enhanced synthetic data from Redis (not real ticks)")
            print("üí° To get REAL data: Run real-time collection or provide CSV dataset")
            return df_redis
    except Exception as e:
        print(f"Error loading from Redis: {e}")

    # 4. Try recent trades from Bybit API (semi-real but limited)
    print("Synthetic data failed, trying recent trades from Bybit API...")
    try:
        df_trades = fetch_bybit_recent_trades(symbol, limit=1000)  # Get more trades
        if not df_trades.empty:
            # Enhanced bid/ask volume inference
            df_trades['bid_vol'] = df_trades['volume'] * np.where(df_trades['side'] == 'Buy', 0.7, 0.3)
            df_trades['ask_vol'] = df_trades['volume'] - df_trades['bid_vol']
            df_trades['spread'] = 0.0001
            df_trades['volume'] = df_trades['volume']  # Already have volume column

            print(f"‚ö†Ô∏è Using recent trades (semi-real) - got {len(df_trades)} trades")
            return df_trades

    except Exception as e:
        print(f"Error fetching recent trades: {e}")

    # 5. Final fallback to enhanced simulation
    print("‚ùå All real data sources failed, using enhanced simulation...")
    return simulate_enhanced_tick_data(symbol, days)

def fetch_bybit_recent_trades(symbol="BTCUSDT", limit=500):
    """
    Fetch recent trades from Bybit API.
    Note: Limited to last 500 trades, for full historical tick data,
    consider premium APIs like Polygon.io or downloading historical datasets.
    """
    base_url = "https://api.bybit.com"
    endpoint = "/v5/market/recent-trade"
    params = {
        "category": "linear",
        "symbol": symbol,
        "limit": limit
    }

    try:
        response = requests.get(f"{base_url}{endpoint}", params=params)
        response.raise_for_status()
        data = response.json()

        if data['retCode'] != 0:
            raise ValueError(f"Bybit API error: {data['retMsg']}")

        trades = []
        for trade in data['result']['list']:
            trades.append({
                'timestamp': pd.to_datetime(int(trade['time']), unit='ms'),
                'price': float(trade['price']),
                'volume': float(trade['size']),
                'side': trade['side']  # Buy or Sell
            })

        df = pd.DataFrame(trades)
        df = df.sort_values('timestamp').reset_index(drop=True)
        return df

    except Exception as e:
        print(f"Error fetching Bybit trades: {e}")
        return pd.DataFrame()

def fetch_bybit_klines(symbol="BTCUSDT", interval="1", days=7):
    """
    Fetch OHLCV klines from Bybit API to use as base for enhanced simulation.
    """
    base_url = "https://api.bybit.com"
    endpoint = "/v5/market/kline"
    limit = 1000

    end_time = int(datetime.now().timestamp() * 1000)
    start_time = int((datetime.now() - timedelta(days=days)).timestamp() * 1000)

    params = {
        "category": "linear",
        "symbol": symbol,
        "interval": interval,
        "start": start_time,
        "end": end_time,
        "limit": limit
    }

    try:
        response = requests.get(f"{base_url}{endpoint}", params=params)
        response.raise_for_status()
        data = response.json()

        if data['retCode'] != 0:
            raise ValueError(f"Bybit API error: {data['retMsg']}")

        klines = []
        for kline in data['result']['list']:
            klines.append({
                'timestamp': pd.to_datetime(int(kline[0]), unit='ms'),
                'open': float(kline[1]),
                'high': float(kline[2]),
                'low': float(kline[3]),
                'close': float(kline[4]),
                'volume': float(kline[5])
            })

        df = pd.DataFrame(klines)
        df = df.sort_values('timestamp').reset_index(drop=True)
        return df

    except Exception as e:
        print(f"Error fetching Bybit klines: {e}")
        return pd.DataFrame()

def simulate_enhanced_tick_data(symbol="BTCUSDT", days=7):
    """
    Enhanced simulation using real market data volatility and trends.
    More realistic than pure random walk - incorporates market microstructure.
    """
    print(f"Creating enhanced tick simulation for {symbol}...")

    # Fetch recent OHLCV data for volatility calibration
    klines_df = fetch_bybit_klines(symbol, interval="5", days=days)  # 5-min bars

    if klines_df.empty:
        print("Could not fetch market data, using basic simulation as final fallback")
        # Fall back to basic simulation if API fails
        n_samples = 10000
        prices = np.cumsum(np.random.normal(0, 0.001, n_samples)) + 50000  # BTC-like starting price
    else:
        # Use real price data as base
        real_prices = klines_df['close'].values
        real_returns = np.diff(np.log(real_prices))
        volatility = np.std(real_returns)

        # Simulate more ticks than available klines
        n_klines = len(klines_df)
        ticks_per_kline = 50  # Simulate 50 ticks per 5-min period
        n_samples = n_klines * ticks_per_kline

        # Interpolate prices between klines for more realistic path
        from scipy import interpolate
        times_orig = np.arange(len(real_prices))
        times_new = np.linspace(0, len(real_prices)-1, n_samples)

        # Cubic spline interpolation for smooth price path
        cs = interpolate.interp1d(times_orig, real_prices, kind='cubic')
        prices = cs(times_new)

        # Add realistic micro-volatility
        micro_noise = np.random.normal(0, volatility * 0.1, n_samples)
        prices += micro_noise

    # Use the prices array for simulation
    n_samples = len(prices)
    prices = prices.astype(float)

    # More realistic bid/ask volume simulation
    # In real markets, bid/ask volumes correlate with market direction and volatility
    returns = pd.Series(prices).pct_change().fillna(0).values
    volatility_measure = pd.Series(returns).rolling(50).std().fillna(0.001).values

    # Base volumes with time-of-day patterns
    base_volumes = []
    for i in range(n_samples):
        hour = (i % (24 * 12)) / 12  # Assuming ~5 ticks per minute, 12 per hour
        # Simulate higher volume during active market hours
        tod_multiplier = 1.5 if 9 <= hour <= 16 else 0.7
        base_volumes.append(np.random.exponential(100) * tod_multiplier)

    base_volumes = np.array(base_volumes)

    # Smooth the volumes for realism
    base_volumes = pd.Series(base_volumes).rolling(20).mean().fillna(100).values

    # Bid/ask split based on price direction and volatility
    bid_ratios = 0.5 + 0.1 * np.sin(2 * np.pi * np.arange(n_samples) / 100)  # Oscillating bias

    # Add correlation with returns (buying pressure when rising)
    buy_pressure = np.where(returns > 0, 0.05, -0.05)
    bid_ratios += buy_pressure

    # Volatile markets have more balanced bid/ask
    bid_ratios -= 0.1 * volatility_measure

    # Constrain to realistic ranges
    bid_ratios = np.clip(bid_ratios, 0.3, 0.7)

    bid_vol = base_volumes * bid_ratios
    ask_vol = base_volumes * (1 - bid_ratios)

    # Realistic spread that varies with volatility
    bid_ask_spread = 0.0001 + volatility_measure * 10  # Spread proportional to volatility

    # Create DataFrame with proper timestamp index
    timestamps = pd.date_range(start=datetime.now() - timedelta(days=days),
                             periods=n_samples, freq='200ms')  # ~5 ticks per second

    data = pd.DataFrame({
        'timestamp': timestamps,
        'price': prices,
        'bid_vol': bid_vol,
        'ask_vol': ask_vol,
        'spread': bid_ask_spread,
        'volume': base_volumes
    })

    print(f"Enhanced simulation created {len(data)} ticks")
    return data

def download_sample_data(symbol="BTCUSDT"):
    """
    Example function to show how you might download real tick data.
    In practice, you would use paid APIs or historical datasets.
    """
    print("To get real tick data:")
    print("1. Use Polygon.io API for US stocks/major crypto (premium access for minute-level)")
    print("2. Use Bybit API with WebSocket for real-time order book")
    print("3. Download historical datasets from Kaggle or crypto data providers")
    print("4. Use GitHub's 'orderflow' package for reshaping existing tick data")

    # Example pseudocode for downloading sample data
    # For demo, you could put a sample CSV in data/BTCUSDT/tick_data.csv
    sample_csv_path = f"data/{symbol}/tick_data_sample.csv"
    if not os.path.exists(f"data/{symbol}"):
        os.makedirs(f"data/{symbol}")

    # This would be replaced with actual download logic
    sample_csv_url = "https://example.com/sample_tick_data.csv"
    print(f"For sample data, download and place CSV at: {sample_csv_path}")
    print("Required columns: timestamp, price, bid_vol, ask_vol, spread")

# LSTM Model (unchanged from original)
class OrderFlowLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(OrderFlowLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        out = self.fc(h_n[0])
        return self.sigmoid(out)

def run_full_pipeline(symbol="BTCUSDT", test_backtesting=False):
    """
    Complete ML pipeline from data loading to backtesting
    """
    print("=== Order Flow ML Pipeline ===")

    # Step 1: Load Real Tick Data
    data = load_real_tick_data(symbol)

    if data.empty:
        raise ValueError("Could not load tick data")

    print(f"Loaded {len(data)} tick records")

    # Step 2: Feature Engineering
    print("Computing order flow features...")

    data['Delta'] = data['bid_vol'] - data['ask_vol']
    data['OFI'] = (data['bid_vol'] / data['spread']) - (data['ask_vol'] / data['spread'])
    data['CVD'] = data['Delta'].cumsum()

    mean_vol = data['volume'].mean()
    data['LargeOrder'] = (data['volume'] > 2 * mean_vol).astype(int)

    # Target: Next tick direction
    data['NextDirection'] = (data['price'].shift(-1) > data['price']).astype(int)
    data.dropna(inplace=True)

    print(f"Feature engineering complete. Shape: {data.shape}")

    # RESAMPLE FOR BACKTESTING COMPATIBILITY: Convert tick data to 5-minute candles for both training and backtesting
    print("Resampling tick data to 5T candles for model training...")
    data_resampled = resample_ticks_to_ohclv(data.set_index('timestamp'), '5T')
    if data_resampled.empty:
        raise ValueError("Could not resample data")

    # Add order flow features to candle data
    data_resampled['PrevClose'] = data_resampled['Close'].shift(1)
    data_resampled['PriceChange'] = data_resampled['Close'] - data_resampled['PrevClose']

    # Simulate order flow features for candles
    data_resampled['Delta'] = np.where(data_resampled['PriceChange'] > 0, data_resampled['Volume'], -data_resampled['Volume'])
    data_resampled['OFI'] = data_resampled['Delta'] / data_resampled['Volume'].rolling(5).mean()
    data_resampled['CVD'] = data_resampled['Delta'].cumsum()
    data_resampled['LargeOrder'] = (data_resampled['Volume'] > data_resampled['Volume'].shift(1) * 2).astype(int)

    # Target: Next candle direction (predicts if price will go up)
    data_resampled['NextDirection'] = (data_resampled['Close'].shift(-1) > data_resampled['Close']).astype(int)
    data_resampled = data_resampled.dropna()

    print(f"Resampled to {len(data_resampled)} 5T candles with order flow features")

    # Step 3: Prepare Data for LSTM (now using candle data)
    features = ['Delta', 'OFI', 'CVD', 'LargeOrder']
    X = data_resampled[features].values
    y = data_resampled['NextDirection'].values

    # Scale features
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    # Create time windows
    timesteps = 10
    X_lstm, y_lstm = [], []
    for i in range(timesteps, len(X_scaled)):
        X_lstm.append(X_scaled[i-timesteps:i])
        y_lstm.append(y[i])
    X_lstm = np.array(X_lstm)
    y_lstm = np.array(y_lstm)

    print(f"Created {len(X_lstm)} time windows")

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)

    # Convert to tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1).to(device)

    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

    # Step 4: Train LSTM Model
    print("Training LSTM model...")

    # Create a better model architecture with dropout and multiple layers
    class EnhancedOrderFlowLSTM(nn.Module):
        def __init__(self, input_size, hidden_size, output_size):
            super(EnhancedOrderFlowLSTM, self).__init__()
            self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0.2)
            self.lstm2 = nn.LSTM(hidden_size, hidden_size//2, batch_first=True)
            self.dropout = nn.Dropout(0.3)
            self.fc1 = nn.Linear(hidden_size//2, hidden_size//4)
            self.fc2 = nn.Linear(hidden_size//4, output_size)
            self.relu = nn.ReLU()
            self.sigmoid = nn.Sigmoid()

        def forward(self, x):
            out, (h_n, _) = self.lstm1(x)
            out, (h_n, _) = self.lstm2(out)
            out = self.dropout(h_n[0])
            out = self.relu(self.fc1(out))
            out = self.dropout(out)
            out = self.sigmoid(self.fc2(out))
            return out

    input_size = X_train.shape[2]  # 4 features
    hidden_size = 100  # Increased hidden size
    output_size = 1
    model = EnhancedOrderFlowLSTM(input_size, hidden_size, output_size).to(device)

    # Better training configuration
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.005)  # Higher learning rate
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, min_lr=1e-6)

    epochs = 50  # Much more training
    best_accuracy = 0
    patience = 10
    patience_counter = 0

    train_losses = []
    test_accuracies = []

    print("Starting enhanced training with dropout and scheduler...")

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping
            optimizer.step()
            running_loss += loss.item()
        avg_train_loss = running_loss / len(train_loader)

        # Test accuracy on validation set
        model.eval()
        with torch.no_grad():
            test_outputs = model(X_test_tensor)
            test_predicted = (test_outputs > 0.5).float()
            test_accuracy = (test_predicted == y_test_tensor).float().mean().item()

        train_losses.append(avg_train_loss)
        test_accuracies.append(test_accuracy)

        # Learning rate scheduling based on validation loss
        scheduler.step(avg_train_loss)

        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Test Acc: {test_accuracy:.3f}')

        # Early stopping based on test accuracy
        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy
            patience_counter = 0
            # Save best model (in memory for now)
            best_model_state = model.state_dict()
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1} (best test accuracy: {best_accuracy:.3f})")
            model.load_state_dict(best_model_state)  # Load best model
            break

    # Evaluate
    model.eval()
    with torch.no_grad():
        outputs = model(X_test_tensor)
        predicted = (outputs > 0.5).float()
        accuracy = (predicted == y_test_tensor).float().mean().item()
        print(f'Test Accuracy: {accuracy:.2f}')

    # Step 5: Backtesting Integration (if available)
    global trained_model, feature_scaler  # Make available to backtesting strategy
    trained_model = model
    feature_scaler = scaler

    if BACKTESTING_AVAILABLE and test_backtesting:
        print("Running backtest...")
        # Convert tick data to OHLCV for backtesting
        data_btc = data.set_index('timestamp').copy()
        ohclv_data = resample_ticks_to_ohclv(data_btc, '5T')  # 5-minute candles

        if ohclv_data.empty:
            print("No OHLCV data available for backtesting")
        else:
            # Create backtest instance - pass model and scaler as strategy parameters
            bt = Backtest(ohclv_data, OrderFlowStrategy,
                         cash=10000, commission=.002,
                         exclusive_orders=True)

            # Run backtest
            stats = bt.run()
            print(stats)

            # Plot results
            bt.plot(filename='order_flow_backtest.html')
            print("Backtest results saved as order_flow_backtest.html")

    print("=== Pipeline Complete ===")
    return model, scaler, accuracy

def resample_ticks_to_ohclv(tick_data, freq='5T'):
    """
    Convert tick data to OHLCV candles for backtesting
    """
    try:
        # Group by time periods and compute OHLCV
        ohlcv = tick_data.resample(freq).agg({
            'price': ['first', 'max', 'min', 'last'],
            'volume': 'sum'
        })

        # Flatten column names to match backtesting.py expectations
        ohlcv.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        ohlcv = ohlcv.dropna()

        return ohlcv

    except Exception as e:
        print(f"Error resampling data: {e}")
        return pd.DataFrame()

if __name__ == "__main__":
    # Check if user wants to download sample data
    if len(os.sys.argv) > 1 and os.sys.argv[1] == "download_sample":
        download_sample_data()
    else:
        # Run the full pipeline
        try:
            model, scaler, accuracy = run_full_pipeline(test_backtesting=True)
            print(f"üéâ Final Model Accuracy: {accuracy:.2f}")

            # GPU memory info
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated(0) / 1024**3  # GB
                memory_reserved = torch.cuda.memory_reserved(0) / 1024**3  # GB
                print(f"üíæ GPU Memory Used: {memory_used:.2f} GB")
                print(f"üìä GPU Memory Reserved: {memory_reserved:.2f} GB")
                print(f"üß† Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

        except Exception as e:
            print(f"Error running pipeline: {e}")

            # Help user get data
            print("\nTo get real tick data:")
            print("1. Place a CSV file at data/BTCUSDT/tick_data.csv with columns:")
            print("   timestamp, price, bid_vol, ask_vol, spread")
            print("2. Or run 'python order_flow_ml.py download_sample' for sample data info")
            print("3. For full historical data, consider Polygon.io or historical crypto datasets")
