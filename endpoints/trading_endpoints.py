# Trading-related API endpoints

import json
import glob
import pandas as pd
import numpy as np
from datetime import datetime, timezone
from fastapi import Request, HTTPException, Depends
from fastapi.responses import JSONResponse
from config import SUPPORTED_SYMBOLS
from auth import require_valid_certificate
from indicators import find_buy_signals, get_timeframe_seconds
from logging_config import logger

from redis_utils import get_cached_klines, cache_klines, fetch_klines_from_bybit
from indicators import _prepare_dataframe


async def get_agent_trades_endpoint(symbol: str, from_ts: int, to_ts: int):
    """
    Retrieves trade records from episode CSV files generated by gemini_RL.py.
    """
    # The symbol is not in the CSV, but we accept it for API consistency.
    logger.info(f"GET /get_agent_trades request for symbol={symbol} from {from_ts} to {to_ts}")

    try:
        # Find all episode CSV files in the current directory
        episode_files = glob.glob("episode_*.csv")
        if not episode_files:
            logger.warning("No episode CSV files found for /get_agent_trades.")
            return JSONResponse(content={"status": "no_data", "trades": [], "message": "No trade data files found."}, status_code=404)

        # Sort to find the latest episode file (e.g., episode_0025.csv > episode_0024.csv)
        episode_files.sort()
        latest_episode_file = episode_files[-1]
        logger.info(f"Reading latest agent trades from: {latest_episode_file}")

        all_trades_df_list = []
        try:
            # Read only the latest file
            df = pd.read_csv(latest_episode_file, low_memory=False)
            all_trades_df_list.append(df)
        except pd.errors.EmptyDataError:
            logger.warning(f"Latest episode file is empty: {latest_episode_file}")
            # This will result in an empty list, which is handled below
        except Exception as e:
            logger.error(f"Error reading latest episode file {latest_episode_file}: {e}")
            # This will also result in an empty list

        if not all_trades_df_list:
            logger.warning("All found episode CSV files were empty or unreadable.")
            return JSONResponse(content={"status": "no_data", "trades": [], "message": "Trade data files are empty or unreadable."}, status_code=404)

        combined_df = pd.concat(all_trades_df_list, ignore_index=True)

        if 'timestamp' not in combined_df.columns or 'action' not in combined_df.columns:
            logger.error("Required 'timestamp' or 'action' column not found in combined CSV data.")
            return JSONResponse(content={"status": "error", "message": "Required columns missing in data files."}, status_code=500)

        combined_df['timestamp'] = pd.to_numeric(combined_df['timestamp'], errors='coerce')
        combined_df.dropna(subset=['timestamp'], inplace=True)

        trade_actions = ['buy', 'sell', 'close_long', 'close_short']
        trades_df = combined_df[combined_df['action'].isin(trade_actions)].copy()
        trades_df = trades_df[(trades_df['timestamp'] >= from_ts) & (trades_df['timestamp'] <= to_ts)]
        trades_df.sort_values(by='timestamp', inplace=True)
        # Replace NaN with None for JSON compatibility before converting to dict
        trades_df_cleaned = trades_df.replace({np.nan: None})
        trades_list = trades_df_cleaned.to_dict(orient='records')
        logger.info(f"Found {len(trades_list)} trades for {symbol} in the specified time range.")
        logger.debug(f"Trades data (first 5 shown if many): {trades_list[:5] if len(trades_list) > 5 else trades_list}")

        return JSONResponse(content={"status": "success", "trades": trades_list})
    except Exception as e:
        logger.error(f"Error in /get_trades endpoint: {e}", exc_info=True)
        return JSONResponse(content={"status": "error", "message": "An unexpected error occurred while fetching trades."}, status_code=500)

async def get_order_history_endpoint(symbol: str, request: Request, cert_check: bool = Depends(require_valid_certificate)):
    """
    Fetches and returns the current open positions for a given symbol from Bybit.
    This endpoint is protected and requires a valid client certificate.
    """
    logger.info(f"GET /get_order_history/{symbol} request received.")
    if symbol not in SUPPORTED_SYMBOLS:
        logger.warning(f"GET /get_order_history: Unsupported symbol: {symbol}")
        return JSONResponse({"status": "error", "message": f"Unsupported symbol: {symbol}"}, status_code=400)

    email = request.session.get("email")
    if not email:
        logger.warning("GET /get_order_history: Email not found in session.")
        return JSONResponse({"status": "error", "message": "Email not found in session"}, status_code=400)

    try:
        from redis_utils import get_redis_connection
        from auth import creds
        from time_sync import sync_time_with_ntp
        from pybit.unified_trading import HTTP

        session = HTTP(
            api_key=creds.api_key,
            api_secret=creds.api_secret,
            testnet=False,
            recv_window=20000,
            max_retries=1
        )

        redis = await get_redis_connection()
        # Define a key prefix for storing order history in Redis, unique per email and symbol
        order_history_key_prefix = f"order_history:{email}:{symbol}"

        # Fetch existing order history from Redis
        existing_order_history = []
        redis_keys = await redis.keys(f"{order_history_key_prefix}:*")
        if redis_keys:
            stored_orders = await redis.mget(*redis_keys)
            if stored_orders:
                existing_order_history = [json.loads(order) for order in stored_orders]

        # Create a set of existing order IDs to efficiently check for duplicates
        existing_order_ids = {order['orderId'] for order in existing_order_history}

        # Retrieve recent order history from Bybit API
        '''
        res = session.get_order_history(
            category="linear",
            symbol=symbol,
            limit=200  # Adjust limit as needed
        )
        '''
        # With this:
        res = session.get_open_orders(
            category="linear",
            symbol=symbol
        )

        if res.get("retCode") != 0:
            error_message = res.get('retMsg', 'Unknown Bybit API error')
            logger.error(f"Bybit API error fetching order history for {symbol}: {error_message}")
            return JSONResponse({"status": "error", "message": error_message}, status_code=500)

        new_orders = res.get("result", {}).get("list", [])

        if res.get("retCode") == 0:
            orderHistory = res.get("result", {}).get("list", [])
            orders_to_persist = []
            for h in orderHistory:
                # Filter out already persisted order ID's and combine with history from Bybit
                orders_to_persist = [order for order in new_orders if order.get('orderId') not in existing_order_ids]

            persisted_count = 0
            async with redis.pipeline() as pipe:
                for order in orders_to_persist:
                    # Create a unique key for each order
                    order_id = order.get('orderId')
                    if order_id:
                        order_key = f"{order_history_key_prefix}:{order_id}"
                        order_json = json.dumps(order)
                        # Set the order data in Redis with an expiration time (e.g., 30 days)
                        await pipe.setex(order_key, 30 * 24 * 3600, order_json)  # type: ignore # Persist each trade history 30 days
                        persisted_count += 1
                    else:
                        logger.warning("Order ID wasn't found")

                await pipe.execute()

            if persisted_count > 0:
                logger.info(f"Persisted {persisted_count} new orders from Bybit API to Redis for {email} and symbol {symbol}")

            # Fetch all order history from Redis after persisting new ones
            all_order_history = []
            redis_keys = await redis.keys(f"{order_history_key_prefix}:*")

            if redis_keys:
                stored_orders = await redis.mget(*redis_keys)
                if stored_orders:
                    all_order_history = [json.loads(order) for order in stored_orders]

            logger.info(f"Returning {len(all_order_history)} order history records from Redis for {email} and symbol {symbol}")
            logger.info(all_order_history)
            return JSONResponse({"status": "success", "order history": all_order_history})

        else:
            # Check for Bybit API errors that might indicate time synchronization issues
            # retCode 10002 is a direct timestamp/recv_window error
            if res.get("retCode") == 10002:
                logger.warning("Received timestamp error from Bybit. Attempting time synchronization with NTP...")
                if sync_time_with_ntp():
                    # Retry the request after time synchronization
                    return await get_order_history_endpoint(symbol)
                else:
                    logger.error("Time synchronization failed. Cannot retry Bybit API request.")

            error_message = res.get('retMsg', 'Unknown Bybit API error')
            logger.error(f"Bybit API error fetching open trades for {symbol}: {error_message}")
            return JSONResponse({"status": "error", "message": error_message}, status_code=500)

    except Exception as e:
        logger.error(f"Error fetching open trades for {symbol}: {e}", exc_info=True)
        return JSONResponse({"status": "error", "message": "An unexpected error occurred"}, status_code=500)

async def get_buy_signals_endpoint(symbol: str, resolution: str, from_ts: int, to_ts: int):
    """
    Analyzes historical data for a symbol to find moments that match the
    "buy the dip in a downtrend" criteria.
    """
    # Convert timestamps to human-readable format for logging
    from_dt_str = datetime.fromtimestamp(from_ts, timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
    to_dt_str = datetime.fromtimestamp(to_ts, timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
    logger.info(f"GET /get_buy_signals/{symbol} request: res={resolution}, from={from_dt_str}, to={to_dt_str}")
    # Validation
    if symbol not in SUPPORTED_SYMBOLS:
        raise HTTPException(status_code=400, detail="Unsupported symbol")

    # Calculate lookback needed for indicators (EMA 200 is the longest)
    lookback_periods = 200 + 50  # 200 for EMA, 50 as a buffer
    timeframe_secs = get_timeframe_seconds(resolution)
    fetch_start_ts = from_ts - (lookback_periods * timeframe_secs)

    signals = []

    try:

        # Fetch klines with lookback
        klines_for_calc = await get_cached_klines(symbol, resolution, fetch_start_ts, to_ts)
        # Check if cache is missing data at the start
        if not klines_for_calc or klines_for_calc[0]['time'] > fetch_start_ts:
            logger.info(f"Cache miss or insufficient lookback for {symbol}. Fetching from Bybit.")
            bybit_klines = fetch_klines_from_bybit(symbol, resolution, fetch_start_ts, to_ts)
            if bybit_klines:
                await cache_klines(symbol, resolution, bybit_klines)
                # Re-query from cache to get a consolidated list
                klines_for_calc = await get_cached_klines(symbol, resolution, fetch_start_ts, to_ts)

        if not klines_for_calc:
            return JSONResponse({"status": "error", "message": "Not enough historical data to perform analysis."}, status_code=404)

        # Prepare DataFrame
        df = pd.DataFrame(klines_for_calc)
        df['time'] = pd.to_datetime(df['time'], unit='s')
        df.set_index('time', inplace=True)
        df.rename(columns={'vol': 'volume'}, inplace=True)
        # pandas_ta needs lowercase ohlcv
        df.columns = [col.lower() for col in df.columns]

        # Add all necessary indicators for the signal logic
        df.ta.ema(length=21, append=True)
        df.ta.ema(length=50, append=True)
        df.ta.ema(length=200, append=True)
        df.ta.rsi(length=14, append=True)
        df.ta.stochrsi(rsi_length=60, length=60, k=10, d=10, append=True)
        df.ta.macd(fast=12, slow=26, signal=9, append=True)  # Add MACD calculation

        # Calculate SMAs for RSI
        df[f'RSI_SMA_5'] = df['RSI_14'].rolling(window=5).mean()
        df[f'RSI_SMA_10'] = df['RSI_14'].rolling(window=10).mean()
        df[f'RSI_SMA_3'] = df['RSI_14'].rolling(window=3).mean()
        df[f'RSI_SMA_20'] = df['RSI_14'].rolling(window=20).mean()

        # Drop rows with NaNs that result from indicator calculations
        df.dropna(subset=[f'EMA_200', 'RSI_14', 'STOCHRSIk_60_60_10_10', 'STOCHRSId_60_60_10_10'], inplace=True)

        if df.empty:
            logger.warning(f"DataFrame for {symbol} became empty after indicator calculation and dropna.")
            return JSONResponse({"status": "error", "message": "Not enough data to calculate all indicators for the requested range."}, status_code=500)

        # Find all signals in the calculated (extended) range
        signals = find_buy_signals(df)
        return JSONResponse({"status": "success", "signals": signals})

    except Exception as e:
        logger.error(f"Error in /get_buy_signals endpoint for {symbol}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An unexpected error occurred during signal analysis.")